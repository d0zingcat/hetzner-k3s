{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"hetzner-k3s The simplest and quickest way to set upproduction-ready Kubernetes clusters on Hetzner Cloud."},{"location":"#what-is-this","title":"What is this?","text":"<p>This is a CLI tool designed to make it incredibly fast and easy to create and manage Kubernetes clusters on Hetzner Cloud (referral link, we both receive some credits) using k3s, a lightweight Kubernetes distribution from Rancher. In a test run, I created a 500-node highly available cluster (3 masters, 497 worker nodes) in just under 11 minutes - though this was with only the public network, as private networks are limited to 100 instances per network. I think this might be a world record!</p> <p>Hetzner Cloud is an awesome cloud provider that offers excellent service with the best performance-to-cost ratio available. They have data centers in Europe, USA and Singapore, making it a versatile choice.</p> <p>k3s is my go-to Kubernetes distribution because it's lightweight, using far less memory and CPU, which leaves more resources for your workloads. It is also incredibly fast to deploy and upgrade because, thanks to being a single binary.</p> <p>With <code>hetzner-k3s</code>, setting up a highly available k3s cluster with 3 master nodes and 3 worker nodes takes only 2-3 minutes. This includes:</p> <ul> <li>Creating all the necessary infrastructure resources (instances, placement groups, load balancer, private network, and firewall).</li> <li>Deploying k3s to the nodes.</li> <li>Installing the Hetzner Cloud Controller Manager to provision load balancers immediately.</li> <li>installing the Hetzner CSI Driver to handle persistent volumes using Hetzner's block storage.</li> <li>Installing the Rancher System Upgrade Controller to simplify and speed up k3s version upgrades.</li> <li>Installing the Cluster Autoscaler to enable autoscaling of node pools.</li> </ul>"},{"location":"Contributing_and_support/","title":"Contributing and support","text":"<p>Feel free to create a pull request if you\u2019d like to suggest any changes. If you\u2019re running into issues with the tool, please open an issue, and I\u2019ll do my best to assist you.</p> <p>If you\u2019re interested in supporting the project financially, you might want to consider becoming a sponsor.</p>"},{"location":"Contributing_and_support/#building-from-source","title":"Building from source","text":"<p>This tool is built using Crystal. If you want to build it or make changes to the code and test them, you\u2019ll need to have Crystal installed on your local machine or use a container.</p> <p>In this repository, you\u2019ll find a Dockerfile that creates a container image with Crystal and all the necessary dependencies. There\u2019s also a Docker Compose file to easily run a container with that image and link the source code into the container. Additionally, there\u2019s a devcontainer.json file that works with compatible IDEs, such as Visual Studio Code, when using the Dev Containers extension.</p>"},{"location":"Contributing_and_support/#developing-with-vscode","title":"Developing with VSCode","text":"<p>To get started, you\u2019ll need to install Visual Studio Code and the Dev Containers extension. Once you have both, open the project in VSCode. You can do this by running <code>code .</code> in the root directory of the git repository.</p> <p>When the project is open, you should see a pop-up dialog asking you to \"Reopen in Container.\" Go ahead and click that option. Wait for the build process to finish and the server to start. After that, click the \"+\" button to open a terminal inside the container.</p> <p>One thing to keep in mind: if you can\u2019t find the Dev Containers extension in the Marketplace (for example, if the first result is the Docker extension instead), make sure you\u2019re using the official build of VSCode. It seems that some extensions are disabled if you\u2019re using an Open Source build.</p>"},{"location":"Contributing_and_support/#developing-with-compose","title":"Developing with Compose","text":"<p>If you prefer not to install VSCode, you can still develop using Docker and Compose in the exact same container.</p> <p>To build and start the development container, use this command: <pre><code>docker compose up -d\n</code></pre></p> <p>After that, to access the container, run: <pre><code>docker compose exec hetzner-k3s bash\n</code></pre></p>"},{"location":"Contributing_and_support/#working-inside-the-container","title":"Working Inside the Container","text":"<p>Once you\u2019re inside the development container (whether through VSCode or Docker Compose directly), you can execute <code>hetzner-k3s</code> like this: <pre><code>crystal run ./src/hetzner-k3s.cr -- create --config cluster_config.yaml\n</code></pre></p> <p>If you want to generate a binary, use this command: <pre><code>crystal build ./src/hetzner-k3s.cr --static\n</code></pre></p> <p>The <code>--static</code> flag ensures the binary is statically linked, meaning it won\u2019t rely on external libraries that might not be available on the system where you plan to run it.</p>"},{"location":"Creating_a_cluster/","title":"Creating a cluster","text":"<p>The tool needs a basic configuration file, written in YAML format, to handle tasks like creating, upgrading, or deleting clusters. Below is an example where commented lines indicate optional settings:</p> <pre><code>---\nhetzner_token: &lt;your token&gt;\ncluster_name: test\nkubeconfig_path: \"./kubeconfig\"\nk3s_version: v1.30.3+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: false # set to true if your key has a passphrase\n    public_key_path: \"~/.ssh/id_ed25519.pub\"\n    private_key_path: \"~/.ssh/id_ed25519\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api: # this will firewall port 6443 on the nodes\n      - 0.0.0.0/0\n  public_network:\n    ipv4: true\n    ipv6: true\n    # hetzner_ips_query_server_url: https://.. # for large clusters, see https://github.com/vitobotta/hetzner-k3s/blob/main/docs/Recommendations.md\n    # use_local_firewall: false # for large clusters, see https://github.com/vitobotta/hetzner-k3s/blob/main/docs/Recommendations.md\n  private_network:\n    enabled: true\n    subnet: 10.0.0.0/16\n    existing_network_name: \"\"\n  cni:\n    enabled: true\n    encryption: false\n    mode: flannel\n\n  # cluster_cidr: 10.244.0.0/16 # optional: a custom IPv4/IPv6 network CIDR to use for pod IPs\n  # service_cidr: 10.43.0.0/16 # optional: a custom IPv4/IPv6 network CIDR to use for service IPs. Warning, if you change this, you should also change cluster_dns!\n  # cluster_dns: 10.43.0.10 # optional: IPv4 Cluster IP for coredns service. Needs to be an address from the service_cidr range\n\n\n# manifests:\n#   cloud_controller_manager_manifest_url: \"https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/v1.23.0/ccm-networks.yaml\"\n#   csi_driver_manifest_url: \"https://raw.githubusercontent.com/hetznercloud/csi-driver/v2.12.0/deploy/kubernetes/hcloud-csi.yml\"\n#   system_upgrade_controller_deployment_manifest_url: \"https://github.com/rancher/system-upgrade-controller/releases/download/v0.14.2/system-upgrade-controller.yaml\"\n#   system_upgrade_controller_crd_manifest_url: \"https://github.com/rancher/system-upgrade-controller/releases/download/v0.14.2/crd.yaml\"\n#   cluster_autoscaler_manifest_url: \"https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/hetzner/examples/cluster-autoscaler-run-on-master.yaml\"\n#   cluster_autoscaler_container_image_tag: \"v1.32.0\"\n\ndatastore:\n  mode: etcd # etcd (default) or external\n  external_datastore_endpoint: postgres://....\n\nschedule_workloads_on_masters: false\n\n# image: rocky-9 # optional: default is ubuntu-24.04\n# autoscaling_image: 103908130 # optional, defaults to the `image` setting\n# snapshot_os: microos # optional: specified the os type when using a custom snapshot\n\nmasters_pool:\n  instance_type: cpx21\n  instance_count: 3 # for HA; you can also create a single master cluster for dev and testing (not recommended for production)\n  locations: # You can choose a single location for single master clusters or if you prefer to have all masters in the same location. For regional clusters (which are only available in the eu-central network zone), each master needs to be placed in a separate location.\n    - fsn1\n    - hel1\n    - nbg1\n\nworker_node_pools:\n- name: small-static\n  instance_type: cpx21\n  instance_count: 4\n  location: hel1\n  # image: debian-11\n  # labels:\n  #   - key: purpose\n  #     value: blah\n  # taints:\n  #   - key: something\n  #     value: value1:NoSchedule\n- name: medium-autoscaled\n  instance_type: cpx31\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 0\n    max_instances: 3\n\n# cluster_autoscaler:\n#   scan_interval: \"10s\"                        # How often cluster is reevaluated for scale up or down\n#   scale_down_delay_after_add: \"10m\"           # How long after scale up that scale down evaluation resumes\n#   scale_down_delay_after_delete: \"10s\"        # How long after node deletion that scale down evaluation resumes\n#   scale_down_delay_after_failure: \"3m\"        # How long after scale down failure that scale down evaluation resumes\n#   max_node_provision_time: \"15m\"              # Maximum time CA waits for node to be provisioned\n\nembedded_registry_mirror:\n  enabled: false # Enables fast p2p distribution of container images between nodes for faster pod startup. Check if your k3s version is compatible before enabling this option. You can find more information at https://docs.k3s.io/installation/registry-mirror\n\nprotect_against_deletion: true\n\ncreate_load_balancer_for_the_kubernetes_api: false # Just a heads up: right now, we can\u2019t limit access to the load balancer by IP through the firewall. This feature hasn\u2019t been added by Hetzner yet.\n\nk3s_upgrade_concurrency: 1 # how many nodes to upgrade at the same time\n\n# additional_packages:\n# - somepackage\n\n# additional_pre_k3s_commands:\n# - apt update\n# - apt upgrade -y\n\n# additional_post_k3s_commands:\n# - apt autoremove -y\n\n# kube_api_server_args:\n# - arg1\n# - ...\n# kube_scheduler_args:\n# - arg1\n# - ...\n# kube_controller_manager_args:\n# - arg1\n# - ...\n# kube_cloud_controller_manager_args:\n# - arg1\n# - ...\n# kubelet_args:\n# - arg1\n# - ...\n# kube_proxy_args:\n# - arg1\n# - ...\n# api_server_hostname: k8s.example.com # optional: DNS for the k8s API LoadBalancer. After the script has run, create a DNS record with the address of the API LoadBalancer.\n</code></pre> <p>Most settings are straightforward and easy to understand. To see a list of available k3s releases, you can run the command <code>hetzner-k3s releases</code>.</p> <p>If you prefer not to include the Hetzner token directly in the config file\u2014perhaps for use with CI or to safely commit the config to a repository\u2014you can use the <code>HCLOUD_TOKEN</code> environment variable instead. This variable takes precedence over the config file.</p> <p>When setting <code>masters_pool</code>.<code>instance_count</code>, keep in mind that if you set it to 1, the tool will create a control plane that is not highly available. For production clusters, it\u2019s better to set this to a number greater than 1. To avoid split brain issues with etcd, this number should be odd, and 3 is the recommended value. Additionally, for production environments, it\u2019s a good idea to configure masters in different locations using the <code>masters_pool</code>.<code>locations</code> setting.</p> <p>You can define any number of worker node pools, either static or autoscaled, and create pools with nodes of different specifications to handle various workloads.</p> <p>Hetzner Cloud init settings, such as <code>additional_packages</code>, <code>additional_pre_k3s_commands</code>, and <code>additional_post_k3s_commands</code>, can be specified at the root level of the configuration file or for each individual pool if different settings are needed. If these settings are configured at the pool level, they will override any settings defined at the root level.</p> <p>The <code>additional_pre_k3s_commands</code> are executed before k3s installation, while <code>additional_post_k3s_commands</code> run after k3s is installed and configured.</p> <p>Currently, Hetzner Cloud offers six locations: two in Germany (<code>nbg1</code> in Nuremberg and <code>fsn1</code> in Falkenstein), one in Finland (<code>hel1</code> in Helsinki), two in the USA (<code>ash</code> in Ashburn, Virginia and <code>hil</code> in Hillsboro, Oregon), and one in Singapore (<code>sin</code>). Be aware that not all instance types are available in every location, so it\u2019s a good idea to check the Hetzner site and their status page for details.</p> <p>To explore the available instance types and their specifications, you can either check them manually when adding an instance within a project or run the following command with your Hetzner token:</p> <pre><code>curl -H \"Authorization: Bearer $API_TOKEN\" 'https://api.hetzner.cloud/v1/server_types'\n</code></pre> <p>To create the cluster run:</p> <pre><code>hetzner-k3s create --config cluster_config.yaml | tee create.log\n</code></pre> <p>This process will take a few minutes, depending on how many master and worker nodes you have.</p>"},{"location":"Creating_a_cluster/#disabling-public-ips-ipv4-or-ipv6-or-both-on-nodes","title":"Disabling public IPs (IPv4 or IPv6 or both) on nodes","text":"<p>To improve security and save on IPv4 address costs, you can disable the public interface for all nodes by setting <code>enable_public_net_ipv4: false</code> and <code>enable_public_net_ipv6: false</code>. These settings are global and will apply to all master and worker nodes. If you disable public IPs, make sure to run hetzner-k3s from a machine that has access to the same private network as the nodes, either directly or through a VPN.</p> <p>Additional networking setup is required via cloud-init, so it\u2019s important that the machine you use to run hetzner-k3s has internet access and DNS configured correctly. Otherwise, the cluster creation process will get stuck after creating the nodes. For more details and instructions, you can refer to this discussion.</p>"},{"location":"Creating_a_cluster/#using-alternative-os-images","title":"Using alternative OS images","text":"<p>By default, the image used for all nodes is <code>ubuntu-24.04</code>, but you can specify a different default image by using the root-level <code>image</code> config option. You can also set different images for different static node pools by using the <code>image</code> config option within each node pool. For example, if you have node pools with ARM instances, you can specify the correct OS image for ARM. To do this, set <code>image</code> to <code>103908130</code> with the specific image ID.</p> <p>However, for autoscaling, there\u2019s a current limitation in the Cluster Autoscaler for Hetzner. You can\u2019t specify different images for each autoscaled pool yet. For now, if you want to use a different image for all autoscaling pools, you can set the <code>autoscaling_image</code> option to override the default <code>image</code> setting.</p> <p>To see the list of available images, run the following:</p> <pre><code>export API_TOKEN=...\n\ncurl -H \"Authorization: Bearer $API_TOKEN\" 'https://api.hetzner.cloud/v1/images?per_page=100'\n</code></pre> <p>Besides the default OS images, you can also use a snapshot created from an existing instance. When using custom snapshots, make sure to specify the ID of the snapshot or image, not the description you assigned when creating the template instance.</p> <p>I\u2019ve tested snapshots with openSUSE MicroOS, but other options might work as well. You can easily create a MicroOS snapshot using this Terraform-based tool. The process only takes a few minutes. Once the snapshot is ready, you can use it with hetzner-k3s by setting the <code>image</code> configuration option to the ID of the snapshot and <code>snapshot_os</code> to <code>microos</code>.</p>"},{"location":"Creating_a_cluster/#keeping-a-project-per-cluster","title":"Keeping a Project per Cluster","text":"<p>If you plan to create multiple clusters within the same project, refer to the section on Configuring Cluster-CIDR and Service-CIDR. Ensure that each cluster has its own unique Cluster-CIDR and Service-CIDR. Overlapping ranges will cause issues. However, I still recommend separating clusters into different projects. This makes it easier to clean up resources\u2014if you want to delete a cluster, simply delete the entire project.</p>"},{"location":"Creating_a_cluster/#configuring-cluster-cidr-and-service-cidr","title":"Configuring Cluster-CIDR and Service-CIDR","text":"<p>Cluster-CIDR and Service-CIDR define the IP ranges used for pods and services, respectively. In most cases, you won\u2019t need to change these values. However, advanced setups might require adjustments to avoid network conflicts.</p> <p>Changing the Cluster-CIDR (Pod IP Range): To modify the Cluster-CIDR, uncomment or add the <code>cluster_cidr</code> option in your cluster configuration file and specify a valid CIDR notation for the network. Make sure this network is not a subnet of your private network.</p> <p>Changing the Service-CIDR (Service IP Range): To adjust the Service-CIDR, uncomment or add the <code>service_cidr</code> option in your configuration file and provide a valid CIDR notation. Again, ensure this network is not a subnet of your private network. Also, uncomment the <code>cluster_dns</code> option and provide a single IP address from the <code>service_cidr</code> range. This sets the IP address for the coredns service.</p> <p>Sizing the Networks: The networks you choose should have enough space for your expected number of pods and services. By default, <code>/16</code> networks are used. Select an appropriate size, as changing the CIDR later is not supported.</p>"},{"location":"Creating_a_cluster/#autoscaler-configuration","title":"Autoscaler Configuration","text":"<p>The cluster autoscaler automatically manages the number of worker nodes in your cluster based on resource demands. When you enable autoscaling for a worker node pool, you can also configure various timing parameters to fine-tune its behavior.</p>"},{"location":"Creating_a_cluster/#basic-autoscaling-configuration","title":"Basic Autoscaling Configuration","text":"<pre><code>worker_node_pools:\n- name: autoscaled-pool\n  instance_type: cpx31\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 10\n</code></pre>"},{"location":"Creating_a_cluster/#advanced-timing-configuration","title":"Advanced Timing Configuration","text":"<p>You can customize the autoscaler's behavior with these optional parameters at the root level of your configuration:</p> <pre><code>cluster_autoscaler:\n  scan_interval: \"2m\"                      # How often cluster is reevaluated for scale up or down\n  scale_down_delay_after_add: \"10m\"        # How long after scale up that scale down evaluation resumes\n  scale_down_delay_after_delete: \"10s\"     # How long after node deletion that scale down evaluation resumes\n  scale_down_delay_after_failure: \"15m\"    # How long after scale down failure that scale down evaluation resumes\n  max_node_provision_time: \"15m\"           # Maximum time CA waits for node to be provisioned\n\nworker_node_pools:\n- name: autoscaled-pool\n  instance_type: cpx31\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 1\n    max_instances: 10\n</code></pre>"},{"location":"Creating_a_cluster/#parameter-descriptions","title":"Parameter Descriptions","text":"<ul> <li><code>scan_interval</code>: Controls how frequently the cluster autoscaler evaluates whether scaling is needed. Shorter intervals mean faster response to load changes but more API calls.</li> <li> <p>Default: <code>10s</code></p> </li> <li> <p><code>scale_down_delay_after_add</code>: Prevents the autoscaler from immediately scaling down after adding nodes. This helps avoid thrashing when workloads are still starting up.</p> </li> <li> <p>Default: <code>10m</code></p> </li> <li> <p><code>scale_down_delay_after_delete</code>: Adds a delay before considering more scale-down operations after a node deletion. This ensures the cluster stabilizes before further changes.</p> </li> <li> <p>Default: <code>10s</code></p> </li> <li> <p><code>scale_down_delay_after_failure</code>: When a scale-down operation fails, this parameter controls how long to wait before attempting another scale-down.</p> </li> <li> <p>Default: <code>3m</code></p> </li> <li> <p><code>max_node_provision_time</code>: Sets the maximum time the autoscaler will wait for a new node to become ready. This is particularly useful for clusters with private networks where provisioning might take longer.</p> </li> <li>Default: <code>15m</code></li> </ul> <p>These settings apply globally to all autoscaling worker node pools in your cluster.</p>"},{"location":"Creating_a_cluster/#idempotency","title":"Idempotency","text":"<p>The <code>create</code> command can be run multiple times with the same configuration without causing issues, as the process is idempotent. If the process gets stuck or encounters errors (e.g., due to Hetzner API unavailability or timeouts), you can stop the command and rerun it with the same configuration to continue where it left off. Note that the kubeconfig will be overwritten each time you rerun the command.</p>"},{"location":"Creating_a_cluster/#limitations","title":"Limitations:","text":"<ul> <li>Using a snapshot instead of a default image will take longer to create instances compared to regular images.</li> <li>The <code>networking</code>.<code>allowed_networks</code>.<code>api</code> setting specifies which networks can access the Kubernetes API, but this currently only works with single-master clusters. Multi-master HA clusters can optionally use a load balancer for the API, but Hetzner\u2019s firewalls do not yet support load balancers.</li> <li>If you enable autoscaling for a nodepool, avoid changing this setting later, as it can cause issues with the autoscaler.</li> <li>Autoscaling is only supported with Ubuntu or other default images, not snapshots.</li> <li>SSH keys with passphrases can only be used if you set <code>networking</code>.<code>ssh</code>.<code>use_ssh_agent</code> to <code>true</code> and use an SSH agent to access your key. For example, on macOS, you can start an agent like this:</li> </ul> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/&lt;private key&gt;\n</code></pre>"},{"location":"Deleting_a_cluster/","title":"Deleting a cluster","text":"<p>To delete a cluster, you need to run the following command:</p> <pre><code>hetzner-k3s delete --config cluster_config.yaml\n</code></pre> <p>This command will remove all the resources in the Hetzner Cloud project that were created by <code>hetzner-k3s</code>.</p> <p>Keep in mind that the load balancers and persistent volumes created by your applications will not be deleted automatically. You\u2019ll need to remove those manually. This might be improved in a future update.</p> <p>Additionally, to delete a cluster, you must ensure that <code>protect_against_deletion</code> is set to <code>false</code>. When you execute the <code>delete</code> command, you\u2019ll also need to enter the cluster\u2019s name to confirm the deletion. These steps are in place to avoid accidentally deleting a cluster you intended to keep.</p>"},{"location":"Floating_IP_egress/","title":"Floating IP egress","text":"<p>To allow for a unique IP for every call getting from your Cluster, enable Cilim egress</p> <pre><code>networking:\n  cni:\n    enabled: true\n    mode: cilium\n    cilium_egress_gateway: true\n</code></pre> <p>Also add a node that will be the middle man</p> <pre><code>worker_node_pools:\n  - name: egress\n    instance_type: cax21\n    location: hel1\n    instance_count: 1\n    autoscaling:\n      enabled: false\n    labels:\n      - key: node.kubernetes.io/role\n        value: \"egress\"\n    taints:\n      - key: node.kubernetes.io/role\n        value: egress:NoSchedule\n</code></pre> <p>Then assign a floating IP to that node.</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumEgressGatewayPolicy\nmetadata:\n  name: egress-global\nspec:\n  selectors:\n    - podSelector: {}\n\n  destinationCIDRs:\n    - \"0.0.0.0/0\"\n  excludedCIDRs:\n    - \"10.0.0.0/8\"\n\n  egressGateway:\n    nodeSelector:\n      matchLabels:\n        node.kubernetes.io/role: egress\n    egressIP: YOUR_FLOATING_IP\n</code></pre> <p>That policy makes it so the outgoing traffic goes from YOUR_FLOATING_IP</p>"},{"location":"Installation/","title":"Installation","text":""},{"location":"Installation/#prerequisites","title":"Prerequisites","text":"<p>To use this tool, you will need a few things:</p> <ul> <li>A Hetzner Cloud account.</li> <li>A Hetzner Cloud token: To get this, create a project in the cloud console, then generate an API token with both read and write permissions (go to the sidebar &gt; Security &gt; API Tokens). Remember, you\u2019ll only see the token once, so make sure to save it somewhere secure.</li> <li>kubectl and Helm installed, as these are necessary for installing components in the cluster and performing k3s upgrades.</li> </ul>"},{"location":"Installation/#installation","title":"Installation","text":""},{"location":"Installation/#macos","title":"macOS","text":""},{"location":"Installation/#with-homebrew","title":"With Homebrew","text":"<pre><code>brew install vitobotta/tap/hetzner_k3s\n</code></pre>"},{"location":"Installation/#binary-installation","title":"Binary installation","text":"<p>First, install these dependencies: - libevent - bdw-gc - libyaml - pcre - gmp</p>"},{"location":"Installation/#apple-silicon-arm","title":"Apple Silicon / ARM","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.3.2/hetzner-k3s-macos-arm64\nchmod +x hetzner-k3s-macos-arm64\nsudo mv hetzner-k3s-macos-arm64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#intel-x86","title":"Intel / x86","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.3.2/hetzner-k3s-macos-amd64\nchmod +x hetzner-k3s-macos-amd64\nsudo mv hetzner-k3s-macos-amd64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#linux","title":"Linux","text":"<p>NOTE: If you're using certain distributions like Fedora, you might run into a little issue when you try to run hetzner-k3s because of a different version of OpenSSL. The easiest way to fix this, for now, is to run these commands before starting hetzner-k3s:</p> <pre><code>export OPENSSL_CONF=/dev/null\nexport OPENSSL_MODULES=/dev/null\n</code></pre>"},{"location":"Installation/#amd64","title":"amd64","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.3.2/hetzner-k3s-linux-amd64\nchmod +x hetzner-k3s-linux-amd64\nsudo mv hetzner-k3s-linux-amd64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#arm","title":"arm","text":"<pre><code>wget https://github.com/vitobotta/hetzner-k3s/releases/download/v2.3.2/hetzner-k3s-linux-arm64\nchmod +x hetzner-k3s-linux-arm64\nsudo mv hetzner-k3s-linux-arm64 /usr/local/bin/hetzner-k3s\n</code></pre>"},{"location":"Installation/#windows","title":"Windows","text":"<p>For Windows, I recommend using the Linux binary with WSL.</p>"},{"location":"Load_balancers/","title":"Load balancers","text":"<p>Once your cluster is ready, you can start provisioning services of type <code>LoadBalancer</code> for your workloads, like the Nginx ingress controller. This is made possible by the Hetzner Cloud Controller Manager, which is installed automatically.</p> <p>To configure the load balancers, you can add annotations to your services. At a minimum, you\u2019ll need these two:</p> <pre><code>load-balancer.hetzner.cloud/location: nbg1  # Ensures the load balancer is in the same network zone as your nodes\nload-balancer.hetzner.cloud/use-private-ip: \"true\"  # Routes traffic between the load balancer and nodes through the private network, avoiding firewall changes\n</code></pre> <p>While the above are essential, I also recommend adding these annotations:</p> <pre><code>load-balancer.hetzner.cloud/hostname: &lt;a valid fqdn&gt;\nload-balancer.hetzner.cloud/http-redirect-https: 'false'\nload-balancer.hetzner.cloud/name: &lt;lb name&gt;\nload-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\n</code></pre> <p>I usually set <code>load-balancer.hetzner.cloud/hostname</code> to a valid hostname, which I configure with the load balancer\u2019s IP after it\u2019s created. I combine this with the annotation <code>load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'</code> to enable the proxy protocol. This is important because it allows my ingress controller and applications to detect the real client IP address. However, enabling the proxy protocol can cause issues with cert-manager failing http01 challenges. To fix this, Hetzner and some other providers recommend using a hostname instead of an IP for the load balancer. For more details, you can read the explanation here. If you want to see the actual client IP, I recommend using these two annotations.</p> <p>The other annotations should be straightforward to understand. For a full list of available annotations, check out this link.</p>"},{"location":"Maintenance/","title":"Maintenance","text":""},{"location":"Maintenance/#adding-nodes","title":"Adding Nodes","text":"<p>To add one or more nodes to a node pool, simply update the instance count in the configuration file for that node pool and run the create command again.</p>"},{"location":"Maintenance/#scaling-down-a-node-pool","title":"Scaling Down a Node Pool","text":"<p>To reduce the size of a node pool:</p> <ol> <li>Lower the instance count in the configuration file to ensure the extra nodes are not recreated in the future.</li> <li>Drain and delete the additional nodes from Kubernetes. These are typically the last nodes when sorted alphabetically by name (<code>kubectl drain Node</code> followed by <code>kubectl delete node &lt;name&gt;</code>).</li> <li>Remove the corresponding instances from the cloud console if the Cloud Controller Manager doesn\u2019t handle this automatically. Make sure you delete the correct ones!</li> </ol>"},{"location":"Maintenance/#replacing-a-problematic-node","title":"Replacing a Problematic Node","text":"<ol> <li>Drain and delete the node from Kubernetes (<code>kubectl drain &lt;name&gt;</code> followed by <code>kubectl delete node &lt;name&gt;</code>).</li> <li>Delete the correct instance from the cloud console.</li> <li>Run the <code>create</code> command again. This will recreate the missing node and add it to the cluster.</li> </ol>"},{"location":"Maintenance/#converting-a-non-ha-cluster-to-ha","title":"Converting a Non-HA Cluster to HA","text":"<p>Converting a single-master, non-HA cluster to a multi-master HA cluster is straightforward. Increase the masters instance count and rerun the <code>create</code> command. This will set up a load balancer for the API server (if enabled) and update the kubeconfig to direct API requests through the load balancer or one of the master contexts. For production clusters, it\u2019s also a good idea to place the masters in different locations (refer to this page for more details).</p>"},{"location":"Maintenance/#replacing-the-seed-master","title":"Replacing the Seed Master","text":"<p>In a new HA cluster, the seed master (or first master) is <code>master1</code>. If you delete <code>master1</code> due to issues and it gets recreated, the seed master will change. When this happens, restart k3s on the existing masters.</p>"},{"location":"Maintenance/#upgrading-to-a-new-version-of-k3s","title":"Upgrading to a New Version of k3s","text":"<p>For the first upgrade of your cluster, simply run the following command to update to a newer version of k3s:</p> <pre><code>hetzner-k3s upgrade --config cluster_config.yaml --new-k3s-version v1.27.1-rc2+k3s1\n</code></pre> <p>Specify the new k3s version as an additional parameter, and the configuration file will be updated automatically during the upgrade. To view available k3s releases, run the command <code>hetzner-k3s releases</code>.</p> <p>Note: For single-master clusters, the API server will be briefly unavailable during the control plane upgrade.</p> <p>To monitor the upgrade progress, use <code>watch kubectl get nodes -owide</code>. You will see the masters upgrading one at a time, followed by the worker nodes.</p>"},{"location":"Maintenance/#what-to-do-if-the-upgrade-doesnt-go-smoothly","title":"What to Do If the Upgrade Doesn\u2019t Go Smoothly","text":"<p>If the upgrade stalls or doesn\u2019t complete for all nodes:</p> <ol> <li>Clean up existing upgrade plans and jobs, then restart the upgrade controller:</li> </ol> <pre><code>kubectl -n system-upgrade delete job --all\nkubectl -n system-upgrade delete plan --all\n\nkubectl label node --all plan.upgrade.cattle.io/k3s-server- plan.upgrade.cattle.io/k3s-agent-\n\nkubectl -n system-upgrade rollout restart deployment system-upgrade-controller\nkubectl -n system-upgrade rollout status deployment system-upgrade-controller\n</code></pre> <p>You can also check the logs of the system upgrade controller\u2019s pod:</p> <pre><code>kubectl -n system-upgrade \\\n  logs -f $(kubectl -n system-upgrade get pod -l pod-template-hash -o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre> <p>If the upgrade stalls after upgrading the masters but before upgrading the worker nodes, simply cleaning up resources might not be enough. In this case, run the following to mark the masters as upgraded and allow the upgrade to continue for the workers:</p> <pre><code>kubectl label node &lt;master1&gt; &lt;master2&gt; &lt;master2&gt; plan.upgrade.cattle.io/k3s-server=upgraded\n</code></pre> <p>Once all the nodes have been upgraded, remember to re-run the <code>hetzner-k3s create</code> command. This way, new nodes will be created with the new version right away. If you don\u2019t, they will first be created with the old version and then upgraded by the system upgrade controller.</p>"},{"location":"Maintenance/#upgrading-the-os-on-nodes","title":"Upgrading the OS on Nodes","text":"<ol> <li>Consider adding a temporary node during the process if your cluster doesn\u2019t have enough spare capacity.</li> <li>Drain one node.</li> <li>Update the OS and reboot the node.</li> <li>Uncordon the node.</li> <li>Repeat for the next node.</li> </ol> <p>To automate this process, you can install the Kubernetes Reboot Daemon (\"Kured\"). For Kured to work effectively, ensure the OS on your nodes has unattended upgrades enabled, at least for security updates. For example, if the image is Ubuntu, add this to the configuration file before running the <code>create</code> command:</p> <pre><code>additional_packages:\n- unattended-upgrades\n- update-notifier-common\nadditional_post_k3s_commands:\n- sudo systemctl enable unattended-upgrades\n- sudo systemctl start unattended-upgrades\n</code></pre> <p>Refer to the Kured documentation for additional configuration options, such as maintenance windows.</p>"},{"location":"Masters_in_different_locations/","title":"Masters in Different Locations","text":"<p>To ensure maximum availability, you can set up a regional cluster by placing each master in a different European location. The first master will be in Falkenstein, the second in Helsinki, and the third in Nuremberg (listed alphabetically). This setup is only possible in network zones with multiple locations, and currently, the only such zone is <code>eu-central</code>, which includes these three European locations. For other regions, only zonal clusters are supported. Regional clusters are limited to 3 masters because we only have these three locations available.</p> <p>To create a regional cluster, set the <code>instance_count</code> for the masters pool to 3 and specify the <code>locations</code> setting as <code>fsn1</code>, <code>hel1</code>, and <code>nbg1</code>.</p>"},{"location":"Masters_in_different_locations/#converting-a-single-master-or-zonal-cluster-to-a-regional-one","title":"Converting a Single Master or Zonal Cluster to a Regional One","text":"<p>If you already have a cluster with a single master or three masters in the same European location, converting it to a regional cluster is straightforward. Just follow these steps carefully and be patient. Note that this requires hetzner-k3s version 2.2.4 or higher.</p> <p>Before you begin, make sure to back up all your applications and data! This is crucial. While the migration process is relatively simple, there is always some level of risk involved.</p> <ul> <li>Set the <code>instance_count</code> for the masters pool to 3 if your cluster currently has only one master.</li> <li>Update the <code>locations</code> setting for the masters pool to include <code>fns1</code>, <code>hel1</code>, and <code>nbg1</code> like this:</li> </ul> <pre><code>locations:\n- fns1\n- hel1\n- nbg1\n</code></pre> <p>The locations are always processed in alphabetical order, regardless of how you list them in the <code>locations</code> property. This ensures consistency, especially when replacing a master due to node failure or other issues.</p> <ul> <li>If your cluster currently has a single master, run the <code>create</code> command with the updated configuration. This will create <code>master2</code> in Helsinki and <code>master3</code> in Nuremberg. Wait for the operation to complete and confirm that all three masters are in a ready state.</li> <li>If <code>master1</code> is not in Falkenstein (fns1):</li> <li>Drain <code>master1</code>.</li> <li>Delete <code>master1</code> using the command <code>kubectl delete node {cluster-name}-master1</code>.</li> <li>Remove the <code>master1</code> instance via the Hetzner Console or the <code>hcloud</code> utility.</li> <li>Run the <code>create</code> command again. This will recreate <code>master1</code> in Falkenstein.</li> <li>SSH into each master and run the following commands to ensure <code>master1</code> has joined the cluster correctly:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install etcd-client\n\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://[REDACTED].1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/server-client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/server-client.key\n\netcdctl member list\n</code></pre> <p>The last command should display something like this if everything is working properly:</p> <pre><code>285ab4b980c2c8c, started, test-master2-d[REDACTED]af, https://[REDACTED]:2380, https://[REDACTED]:2379, false\naad3fac89b68bfb7, started, test-master1-5e550de0, https://[REDACTED]:2380, https://[REDACTED]:2379, false\nc[REDACTED]e25aef34e8, started, test-master3-0ed051a3, https://[REDACTED]:2380, https://[REDACTED]:2379, false\n</code></pre> <ul> <li>If <code>master2</code> is not in Helsinki, follow the same steps as with <code>master1</code> but for <code>master2</code>. This will recreate <code>master2</code> in Helsinki.</li> <li>If <code>master3</code> is not in Nuremberg, repeat the process for <code>master3</code>. This will recreate <code>master3</code> in Nuremberg.</li> </ul> <p>That\u2019s it! You now have a regional cluster, which ensures continued operation even if one of the Hetzner locations experiences a temporary failure. I also recommend enabling the <code>create_load_balancer_for_the_kubernetes_api</code> setting to <code>true</code> if you don\u2019t already have a load balancer for the Kubernetes API.</p>"},{"location":"Masters_in_different_locations/#performance-considerations","title":"Performance Considerations","text":"<p>This feature has been frequently requested, but I delayed implementing it until I could thoroughly test the configuration. I was concerned about latency issues, as etcd is sensitive to delays, and I wanted to ensure that the latency between the German locations and Helsinki wouldn\u2019t cause problems.</p> <p>It turns out that the default heartbeat interval for etcd is 100ms, and the latency between Helsinki and Falkenstein/Nuremberg is only 25-27ms. This means the total round-trip time (RTT) for the Raft consensus is around 60-70ms, which is well within etcd\u2019s acceptable limits. After running benchmarks, everything works smoothly! So, there\u2019s no need to adjust the etcd configuration for this setup.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/","title":"Private clusters with public network interface disabled","text":"<p>By default, network access to nodes in a cluster created with hetzner-k3s is limited to the networks listed in the configuration file. Some users might want to completely turn off the public interface on their nodes instead.</p> <p>This page offers a reference configuration to help you disable the public interface. Keep in mind that some steps might vary depending on the operating system you choose for your nodes. The example configuration has been tested successfully with Debian 12, as it's a bit simpler to work with compared to other OSes.</p> <p>Please note that this configuration is designed for new clusters only. I haven't tested if it works to convert an existing cluster to one with the public interface disabled.</p> <p>When setting up a cluster with disabled public network interfaces, remember you'll need a NAT gateway to access the cluster from outside Hetzner Cloud. Without it, your nodes won't be able to connect to the internet, and hetzner-k3s won\u2019t be able to install k3s on those nodes.</p> <p>Another important thing to consider is that with the public network interface disabled on all nodes, you can't run hetzner-k3s from a computer outside the cluster's private network. So, you'll need to run hetzner-k3s from a cloud instance within the private network. You could use the same instance you're using as your NAT gateway for this purpose too.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/#prerequisite-nat-gateway","title":"Prerequisite: NAT Gateway","text":"<p>First off, you need to set up a NAT gateway for your Hetzner Cloud network. Follow the instructions on this page.</p> <p>The guide uses Debian as an example, so make sure to review the page and adjust any settings according to the operating system you choose for your cluster nodes and the NAT gateway instance.</p> <p>The TL;DR is this:</p> <ul> <li> <p> First, create a private network in the Hetzner project where your cluster will reside. Choose any subnet you like, but for reference purposes, let's assume it\u2019s 10.0.0.0/16.</p> </li> <li> <p> Next, set up a cloud instance to act as your NAT gateway. Ensure it has a public IP address and connects to the private network you just created.</p> </li> <li> <p> Then, add a route on your private network for <code>0.0.0.0/0</code>, directing it to the IP address of your NAT gateway instance, which you can select from a dropdown menu.</p> </li> <li> <p> Finally, on the NAT gateway instance itself, tweak <code>/etc/network/interfaces</code>. Add these lines or adjust existing ones for your private network interface:</p> </li> </ul> <pre><code>auto enp7s0\niface enp7s0 inet dhcp\n    post-up echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n    post-up iptables -t nat -A POSTROUTING -s '10.0.0.0/16' -o enp7s0 -j MASQUERADE\n</code></pre> <p>Replace <code>10.0.0.0/16</code> with your actual subnet if it's different. Also, make sure to use the correct name for your private network interface if <code>enp7s0</code> isn't right\u2014find this with the <code>ifconfig</code> command.</p> <ul> <li> Lastly, restart your NAT gateway instance to apply these changes.</li> </ul>"},{"location":"Private_clusters_with_public_network_interface_disabled/#cluster-configuration","title":"Cluster configuration","text":"<ul> <li> Edit the configuration file for your cluster and set both <code>ipv4</code> and <code>ipv6</code> to <code>false</code>, plus reference the existing private network you have already created:</li> </ul> <pre><code>  public_network:\n    ipv4: false\n    ipv6: false\n  private_network:\n    enabled : true\n    subnet: 10.0.0.0/16\n    existing_network_name: \"&lt;name of your private network&gt;\"\n</code></pre> <p>Also configure the allowed networks:</p> <pre><code>  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api:\n      - 0.0.0.0/0\n</code></pre> <ul> <li> <p> Since you're setting up a private cluster, it makes sense to turn off the load balancer for the Kubernetes API. You can do this by setting <code>create_load_balancer_for_the_kubernetes_api</code> to <code>false</code>.</p> </li> <li> <p> Also, if you want to use an OS image other than the default (<code>ubuntu-24.04</code>), you can configure it accordingly. For example, if you prefer Debian 12, you can set it up like this:</p> </li> </ul> <pre><code>image: debian-12\nautoscaling_image: debian-12\n</code></pre> <ul> <li> Next, you need to set up network configuration commands. These steps will ensure that the nodes in your clusters use the NAT gateway to access the Internet. You can use either <code>additional_pre_k3s_commands</code> (before k3s installation) or <code>additional_post_k3s_commands</code> (after k3s installation) depending on your needs.</li> </ul> <p>For <code>ubuntu-24.04</code> (this should be equivalent to the steps in the hetzner guide):</p> <pre><code>additional_pre_k3s_commands:\n- printf \"[Match]\\nName=enp7s0\\n\\n[Network]\\nDHCP=yes\\nGateway=10.0.0.1\\n\" &gt; /etc/systemd/network/10-enp7s0.network\n- printf \"[Resolve]\\nDNS=185.12.64.2 185.12.64.1\" &gt; /etc/systemd/resolved.conf\n- systemctl restart systemd-networkd\n- systemctl restart systemd-resolved\nadditional_post_k3s_commands:\n# add more steps here if you want to update the instance via apt etc. \n</code></pre> <p>For <code>debian-12</code>:</p> <pre><code>additional_pre_k3s_commands:\n- apt update\n- apt upgrade -y\n- apt install ifupdown resolvconf -y\n- apt autoremove -y hc-utils\n- apt purge -y hc-utils\n- echo \"auto enp7s0\" &gt; /etc/network/interfaces.d/60-private\n- echo \"iface enp7s0 inet dhcp\" &gt;&gt; /etc/network/interfaces.d/60-private\n- echo \"    post-up ip route add default via 10.0.0.1\"  &gt;&gt; /etc/network/interfaces.d/60-private\n- echo \"[Resolve]\" &gt; /etc/systemd/resolved.conf\n- echo \"DNS=1.1.1.1 1.0.0.1\" &gt;&gt; /etc/systemd/resolved.conf\n- ifdown enp7s0\n- ifup enp7s0\n- systemctl start resolvconf\n- systemctl enable resolvconf\n- echo \"nameserver 1.1.1.1\" &gt;&gt; /etc/resolvconf/resolv.conf.d/head\n- echo \"nameserver 1.0.0.1\" &gt;&gt; /etc/resolvconf/resolv.conf.d/head\n- resolvconf --enable-updates\n- resolvconf -u\n</code></pre> <p>Replace <code>enp7s0</code> with your network interface name, and <code>10.0.0.1</code> with the correct gateway IP address for your subnet. Note that this is not the IP address of the NAT gateway instance; it's simply the first IP in the range.</p> <p>One important thing to remember: these simple commands work great if you're using the same type of instances, like all AMD instances, for both your master and worker node pools. We're referencing a specific private network interface name here.</p> <p>If you plan on using different types of instances in your cluster, you'll need to tweak these commands to use a more flexible method for identifying the correct private interface on each node.</p>"},{"location":"Private_clusters_with_public_network_interface_disabled/#creating-the-cluster","title":"Creating the cluster","text":"<p>Run the <code>create</code> command with hetzner-k3s as usual, but use this updated configuration from an instance connected to the same private network. For example, you can use the NAT gateway instance if you don't want to create another one.</p> <p>The nodes will be able to access the Internet through the NAT gateway. Therefore, hetzner-k3s should complete creating the cluster successfully.</p>"},{"location":"Recommendations/","title":"Recommendations","text":""},{"location":"Recommendations/#larger-clusters","title":"Larger Clusters","text":"<p>The default configuration works well for small to medium-sized clusters, so you don\u2019t need to change much if you want a simple, reliable setup..</p> <p>For larger clusters, thought, the default setup is quite limiting. Hetzner\u2019s private networks, which are used in hetzner-k3s' default configuration, only support up to 100 nodes. If you your cluster is going to grow beyond that, I recommend disabling the private network in your configuration.</p> <p>Support for large clusters has gotten a lot better since version 2.2.8. Before that, you could create large clusters using the public network, and all the traffic between the nodes was encrypted and authenticated. However this meant opening the Kubernetes API port on the master nodes and the Wireguard port on all nodes to the public Internet, which introduced some security risks.</p> <p>Here\u2019s what changed in version 2.2.8:</p> <p>Instead of using Hetzner\u2019s firewall, which is slow to update and can be problematic when you\u2019re making a lot of changes at once, a custom firewall was added. This firewall keeps the traffic between nodes secure without opening any ports to the public, unless you specifically want to. One issue with the firewall is that it can\u2019t know the IPs of the nodes in advance, especially when they\u2019re created dynamically or with autoscaling. To solve this, an \"IP query server\" was set up as a simple container. This server checks the Hetzner API every 30 seconds to get the list of all node IPs in the project. Then, the firewall on each node regularly polls this IP query server to update its rules and keep everything secure. This solution is simple and effective, and it means you don\u2019t need to open the Kubernetes API port or the Wireguard port to the public unless you really want to. It also removes the need for manual firewall updates, as everything happens automatically.</p>"},{"location":"Recommendations/#setting-up-the-ip-query-server","title":"Setting up the IP query server","text":"<p>The IP query server runs as a simple container. You can easily set it up on any Docker-enabled server using the <code>docker-compose.yml</code> file in the <code>ip-query-server</code> folder of this repository. This compose project also runs Caddy as a web server, so you can use a domain name with the server. Just replace <code>example.com</code> in the Caddyfile with your actual domain name and <code>mail@example.com</code> with the email address you'll use to request a certificate from Let's Encrypt via Caddy.</p> <p>There's nothing else to configure for the server itself. The firewall on each node sends the server the token for the Hetzner project, which the server uses to get the list of node IPs.</p> <p>Once the server is up and running, change your hetzner-k3s configuration and set <code>networking.public_network.hetzner_ips_query_server_url</code> to your server's URL, and <code>use_local_firewall</code> to <code>true</code>.</p> <p>For a production setup, I recommend having two instances of the server behind a load balancer for better availability.</p>"},{"location":"Recommendations/#additional-notes-about-large-clusters","title":"Additional notes about large clusters","text":"<ul> <li>If you are going to create a large cluster, remember to change the cluster cidr and the service cidr accordingly</li> <li>If you disable the private network due to the node limit, encryption will be applied at the CNI level to secure communication between nodes over the public network.</li> <li>If you prefer a CNI other than Cilium or Flannel (e.g., Calico), you can disable automatic CNI setup and install your preferred CNI manually. We may add support for more CNIs in future releases.</li> <li>Starting with v2.0.0, you can use an external SQL datastore like Postgres instead of the built-in etcd for the Kubernetes API. This can also help with scaling larger clusters.</li> </ul>"},{"location":"Recommendations/#embedded-registry-mirror","title":"Embedded Registry Mirror","text":"<p>In v2.0.0, there\u2019s a new option to enable the <code>embedded registry mirror</code> in k3s. You can find more details here. This feature uses Spegel to enable peer-to-peer distribution of container images across cluster nodes.</p> <p>This can help in situations where nodes face issues pulling images because their IPs have been blocked by registries (due to past misuse or similar reasons). With this setup, a node will first try pulling an image from other nodes via the embedded registry mirror before reaching out to the upstream registry. This not only resolves access issues but also speeds up pod creation, especially for deployments with many replicas spread across multiple nodes. To enable it, set <code>embedded_registry_mirror</code>.<code>enabled</code> to <code>true</code>. Just make sure your k3s version supports this feature by checking the linked page.</p>"},{"location":"Recommendations/#clusters-using-only-the-public-network","title":"Clusters Using Only the Public Network","text":"<p>If you disable the private network to allow your cluster to grow beyond 100 nodes, you won\u2019t be able to restrict access to the Kubernetes API by IP address. This is because the API must be accessible from all nodes, and blocking IPs would prevent communication.</p> <p>This limitation might be addressed in future releases if a workaround is found. For now, the API must be open to 0.0.0.0/0 when the private network is disabled.</p>"},{"location":"Setting_up_a_cluster/","title":"Setting up a cluster","text":"<p>By TitanFighter</p>"},{"location":"Setting_up_a_cluster/#instructions","title":"Instructions","text":""},{"location":"Setting_up_a_cluster/#installation-of-a-hello-world-project","title":"Installation of a \"hello-world\" project","text":"<p>For testing, we\u2019ll use this \"hello-world\" app: hello-world app</p> <ol> <li>Install <code>kubectl</code> on your computer: kubectl installation</li> <li>Install <code>Helm</code> on your computer: Helm installation</li> <li>Install <code>hetzner-k3s</code> on your computer: Installation</li> <li>Create a file called <code>hetzner-k3s_cluster_config.yaml</code> with the following configuration. This setup is for a Highly Available (HA) cluster with 3 master nodes and 3 worker nodes. You can use 1 master and 1 worker for testing:</li> </ol> <pre><code>hetzner_token: ...\ncluster_name: hello-world\nkubeconfig_path: \"./kubeconfig\"  # or /cluster/kubeconfig if you are going to use Docker\nk3s_version: v1.32.0+k3s1\n\nnetworking:\n  ssh:\n    port: 22\n    use_agent: false\n    public_key_path: \"~/.ssh/id_rsa.pub\"\n    private_key_path: \"~/.ssh/id_rsa\"\n  allowed_networks:\n    ssh:\n      - 0.0.0.0/0\n    api:\n      - 0.0.0.0/0\n\nmasters_pool:\n  instance_type: cpx21\n  instance_count: 3\n  locations:\n    - fsn1\n    - hel1\n    - nbg1\n\nworker_node_pools:\n- name: small\n  instance_type: cpx21\n  instance_count: 4\n  location: hel1\n- name: big\n  instance_type: cpx31\n  location: fsn1\n  autoscaling:\n    enabled: true\n    min_instances: 0\n    max_instances: 3\n</code></pre> <p>For more details on all the available settings, refer to the full config example in Creating a cluster.</p> <ol> <li>Create the cluster: <code>hetzner-k3s create --config hetzner-k3s_cluster_config.yaml</code></li> <li><code>hetzner-k3s</code> automatically generates a <code>kubeconfig</code> file for the cluster in the directory where you run the tool. You can either copy this file to <code>~/.kube/config</code> if it\u2019s the only cluster or run <code>export KUBECONFIG=./kubeconfig</code> in the same directory to access the cluster. After this, you can interact with your cluster using <code>kubectl</code> installed in step 1.</li> </ol> <p>TIP: If you don\u2019t want to run <code>kubectl apply ...</code> every time, you can store all your configuration files in a folder and then run <code>kubectl apply -f /path/to/configs/ -R</code>.</p> <ol> <li>Create a file: <code>touch ingress-nginx-annotations.yaml</code></li> <li>Add annotations to the file: <code>nano ingress-nginx-annotations.yaml</code></li> </ol> <pre><code># INSTALLATION\n# 1. Install Helm: https://helm.sh/docs/intro/install/\n# 2. Add ingress-nginx Helm repo: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n# 3. Update information of available charts: helm repo update\n# 4. Install ingress-nginx:\n# helm upgrade --install \\\n# ingress-nginx ingress-nginx/ingress-nginx \\\n# --set controller.ingressClassResource.default=true \\ # Remove this line if you don\u2019t want Nginx to be the default Ingress Controller\n# -f ./ingress-nginx-annotations.yaml \\\n# --namespace ingress-nginx \\\n# --create-namespace\n\n# LIST of all ANNOTATIONS: https://github.com/hetznercloud/hcloud-cloud-controller-manager/blob/master/internal/annotation/load_balancer.go\n\ncontroller:\n  kind: DaemonSet\n  service:\n    annotations:\n      # Germany:\n      # - nbg1 (Nuremberg)\n      # - fsn1 (Falkenstein)\n      # Finland:\n      # - hel1 (Helsinki)\n      # USA:\n      # - ash (Ashburn, Virginia)\n      # Without this, the load balancer won\u2019t be provisioned and will stay in \"pending\" state.\n      # You can check this state using \"kubectl get svc -n ingress-nginx\"\n      load-balancer.hetzner.cloud/location: nbg1\n\n      # Name of the load balancer. This name will appear in your Hetzner cloud console under \"Your project -&gt; Load Balancers\".\n      # NOTE: This is NOT the load balancer created automatically for HA clusters. You need to specify a different name here to create a separate load balancer for ingress Nginx.\n      load-balancer.hetzner.cloud/name: WORKERS_LOAD_BALANCER_NAME\n\n      # Ensures communication between the load balancer and cluster nodes happens through the private network.\n      load-balancer.hetzner.cloud/use-private-ip: \"true\"\n\n      # [ START: Use these annotations if you care about seeing the actual client IP ]\n      # \"uses-proxyprotocol\" enables the proxy protocol on the load balancer so that the ingress controller and applications can see the real client IP.\n      # \"hostname\" is needed if you use cert-manager (LetsEncrypt SSL certificates). It fixes HTTP01 challenges for cert-manager (https://cert-manager.io/docs/).\n      # Check this link for more details: https://github.com/compumike/hairpin-proxy\n      # In short: the easiest fix provided by some providers (including Hetzner) is to configure the load balancer to use a hostname instead of an IP.\n      load-balancer.hetzner.cloud/uses-proxyprotocol: 'true'\n\n      # 1. \"yourDomain.com\" must be correctly configured in DNS to point to the Nginx load balancer; otherwise, certificate provisioning won\u2019t work.\n      # 2. If you use multiple domains, specify any one.\n      load-balancer.hetzner.cloud/hostname: yourDomain.com\n      # [ END: Use these annotations if you care about seeing the actual client IP ]\n\n      load-balancer.hetzner.cloud/http-redirect-https: 'false'\n</code></pre> <ul> <li>Replace <code>yourDomain.com</code> with your actual domain.</li> <li> <p>Replace <code>WORKERS_LOAD_BALANCER_NAME</code> with a name of your choice.</p> </li> <li> <p>Add the ingress-nginx Helm repo: <code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</code></p> </li> <li>Update the Helm repo: <code>helm repo update</code></li> <li>Install ingress-nginx:</li> </ul> <pre><code>helm upgrade --install \\\ningress-nginx ingress-nginx/ingress-nginx \\\n--set controller.ingressClassResource.default=true \\\n-f ./ingress-nginx-annotations.yaml \\\n--namespace ingress-nginx \\\n--create-namespace\n</code></pre> <p>The <code>--set controller.ingressClassResource.default=true</code> flag configures this as the default Ingress Class for your cluster. Without this, you\u2019ll need to specify an Ingress Class for every Ingress object you deploy, which can be tedious. If no default is set and you don\u2019t specify one, Nginx will return a 404 Not Found page because it won\u2019t \"pick up\" the Ingress.</p> <p>TIP: To delete it: <code>helm uninstall ingress-nginx -n ingress-nginx</code>. Be careful, as this will delete the current Hetzner load balancer, and installing a new ingress controller may create a new load balancer with a different public IP.</p> <ol> <li> <p>After a few minutes, check that the \"EXTERNAL-IP\" column shows an IP instead of \"pending\": <code>kubectl get svc -n ingress-nginx</code></p> </li> <li> <p>The <code>load-balancer.hetzner.cloud/uses-proxyprotocol: \"true\"</code> annotation requires <code>use-proxy-protocol: \"true\"</code> for ingress-nginx. To set this up, create a file: <code>touch ingress-nginx-configmap.yaml</code></p> </li> <li>Add the following content to the file: <code>nano ingress-nginx-configmap.yaml</code></li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  # Do not change the name - this is required by the Nginx Ingress Controller\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\ndata:\n  use-proxy-protocol: \"true\"\n</code></pre> <ol> <li>Apply the ConfigMap: <code>kubectl apply -f ./ingress-nginx-configmap.yaml</code></li> <li>Open your Hetzner cloud console, go to \"Your project -&gt; Load Balancers,\" and find the PUBLIC IP of the load balancer with the name you used in the <code>load-balancer.hetzner.cloud/name: WORKERS_LOAD_BALANCER_NAME</code> annotation. Copy or note this IP.</li> <li>Download the hello-world app: <code>curl https://raw.githubusercontent.com/vitobotta/hetzner-k3s/refs/heads/main/sample-deployment.yaml --output hello-world.yaml</code></li> <li>Edit the file to add the annotation and set the hostname:</li> </ol> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-world\n  annotations:                       # &lt;&lt;&lt;--- Add annotation\n    kubernetes.io/ingress.class: nginx  # &lt;&lt;&lt;--- Add annotation\nspec:\n  rules:\n  - host: hello-world.IP_FROM_STEP_12.nip.io # &lt;&lt;&lt;--- Replace `IP_FROM_STEP_12` with the IP from step 16.\n  ....\n</code></pre> <ol> <li>Install the hello-world app: <code>kubectl apply -f hello-world.yaml</code></li> <li> <p>Open http://hello-world.IP_FROM_STEP_12.nip.io in your browser. You should see the Rancher \"Hello World!\" page. The <code>host.IP_FROM_STEP_12.nip.io</code> (the <code>.nip.io</code> part is key) is a quick way to test things without configuring DNS. A query to a hostname ending in <code>.nip.io</code> returns the IP address in the hostname itself. If you enabled the proxy protocol as shown earlier, your public IP address should appear in the <code>X-Forwarded-For</code> header, meaning the application can \"see\" it.</p> </li> <li> <p>To connect your actual domain, follow these steps:</p> </li> <li>Assign the IP address from step 12 to your domain in your DNS settings.</li> <li>Change <code>- host: hello-world.IP_FROM_STEP_12.nip.io</code> to <code>- host: yourDomain.com</code>.</li> <li>Run <code>kubectl apply -f hello-world.yaml</code>.</li> <li>Wait until DNS records are updated.</li> </ol>"},{"location":"Setting_up_a_cluster/#if-you-need-letsencrypt","title":"If you need LetsEncrypt","text":"<ol> <li>Add the LetsEncrypt Helm repo: <code>helm repo add jetstack https://charts.jetstack.io</code></li> <li>Update the Helm repo: <code>helm repo update</code></li> <li>Install the LetsEncrypt certificates issuer:</li> </ol> <pre><code>helm upgrade --install \\\n--namespace cert-manager \\\n--create-namespace \\\n--set crds.enabled=true \\\ncert-manager jetstack/cert-manager\n</code></pre> <ol> <li>Create a file called <code>lets-encrypt.yaml</code> with the following content:</li> </ol> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: cert-manager\nspec:\n  acme:\n    email: [REDACTED]\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod-account-key\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <ol> <li>Apply the file: <code>kubectl apply -f ./lets-encrypt.yaml</code></li> <li>Edit <code>hello-world.yaml</code> and add the settings for TLS encryption:</li> </ol> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-world\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"  # &lt;&lt;&lt;--- Add annotation\n    kubernetes.io/tls-acme: \"true\"                      # &lt;&lt;&lt;--- Add annotation\nspec:\n  rules:\n  - host: yourDomain.com  # &lt;&lt;&lt;---- Your actual domain\n  tls: # &lt;&lt;&lt;---- Add this block\n  - hosts:\n    - yourDomain.com\n    secretName: yourDomain.com-tls # &lt;&lt;&lt;--- Add reference to secret\n  ....\n</code></pre> <p>TIP: If you didn\u2019t configure Nginx as the default Ingress Class, you\u2019ll need to add the <code>spec.ingressClassName: nginx</code> annotation.</p> <ol> <li>Apply the changes: <code>kubectl apply -f ./hello-world.yaml</code></li> </ol>"},{"location":"Setting_up_a_cluster/#faqs","title":"FAQs","text":""},{"location":"Setting_up_a_cluster/#1-can-i-use-metallb-instead-of-hetzners-load-balancer","title":"1. Can I use MetalLB instead of Hetzner's Load Balancer?","text":"<p>Yes, you can use MetalLB with floating IPs in Hetzner Cloud, but I wouldn\u2019t recommend it. The setup with Hetzner's standard load balancers is much simpler. Plus, load balancers aren\u2019t significantly more expensive than floating IPs, so in my opinion, there\u2019s no real benefit to using MetalLB in this case.</p>"},{"location":"Setting_up_a_cluster/#2-how-do-i-create-and-push-docker-images-to-a-repository-and-how-can-kubernetes-work-with-these-images-gitlab-example","title":"2. How do I create and push Docker images to a repository, and how can Kubernetes work with these images? (GitLab example)","text":"<p>On the machine where you create the image:</p> <ul> <li>Start by logging in to the Docker registry: <code>docker login registry.gitlab.com</code>.</li> <li>Build the Docker image: <code>docker build -t registry.gitlab.com/COMPANY_NAME/REPO_NAME:IMAGE_NAME -f /some/path/to/Dockerfile .</code>.</li> <li>Push the image to the registry: <code>docker push registry.gitlab.com/COMPANY_NAME/REPO_NAME:IMAGE_NAME</code>.</li> </ul> <p>On the machine running Kubernetes:</p> <ul> <li>Generate a secret to allow Kubernetes to access the images: <code>kubectl create secret docker-registry gitlabcreds --docker-server=https://registry.gitlab.com --docker-username=MYUSER --docker-password=MYPWD --docker-email=MYEMAIL -n NAMESPACE_OF_YOUR_APP -o yaml &gt; docker-secret.yaml</code>.</li> <li>Apply the secret: <code>kubectl apply -f docker-secret.yaml -n NAMESPACE_OF_YOUR_APP</code>.</li> </ul>"},{"location":"Setting_up_a_cluster/#3-how-can-i-check-the-resource-usage-of-nodes-or-pods","title":"3. How can I check the resource usage of nodes or pods?","text":"<p>First, install the metrics-server from this GitHub repository: https://github.com/kubernetes-sigs/metrics-server. After installation, you can use either <code>kubectl top nodes</code> or <code>kubectl top pods -A</code> to view resource usage.</p>"},{"location":"Setting_up_a_cluster/#4-what-is-ingress","title":"4. What is Ingress?","text":"<p>There are two types of \"ingress\" to understand: <code>Ingress Controller</code> and <code>Ingress Resources</code>.</p> <p>In the case of Nginx:</p> <ul> <li>The <code>Ingress Controller</code> is Nginx itself (defined as <code>kind: Ingress</code>), while <code>Ingress Resources</code> are services (defined as <code>kind: Service</code>).</li> <li>The <code>Ingress Controller</code> has various annotations (rules). You can use these annotations in <code>kind: Ingress</code> to make them \"global\" or in <code>kind: Service</code> to make them \"local\" (specific to that service).</li> <li>The <code>Ingress Controller</code> consists of a Pod and a Service. The Pod runs the Controller, which continuously monitors the /ingresses endpoint in your cluster\u2019s API server for updates to available <code>Ingress Resources</code>.</li> </ul>"},{"location":"Setting_up_a_cluster/#5-how-can-i-configure-autoscaling-to-automatically-set-up-ip-routes-for-new-nodes-to-use-a-nat-server","title":"5. How can I configure autoscaling to automatically set up IP routes for new nodes to use a NAT server?","text":"<p>First, you\u2019ll need a NAT server, as described in this Hetzner community tutorial.</p> <p>Then, use <code>additional_post_k3s_commands</code> to run commands after k3s installation: <pre><code>additional_packages:\n  - ifupdown\nadditional_post_k3s_commands:\n  - apt update\n  - apt upgrade -y\n  - apt autoremove -y\n  - ip route add default via [REDACTED]  # Replace this with your gateway IP\n</code></pre></p> <p>You can also use <code>additional_pre_k3s_commands</code> to run commands before k3s installation if needed.</p>"},{"location":"Setting_up_a_cluster/#useful-commands","title":"Useful Commands","text":"<pre><code>kubectl get service [serviceName] -A or -n [nameSpace]\nkubectl get ingress [ingressName] -A or -n [nameSpace]\nkubectl get pod [podName] -A or -n [nameSpace]\nkubectl get all -A\nkubectl get events -A\nhelm ls -A\nhelm uninstall [name] -n [nameSpace]\nkubectl -n ingress-nginx get svc\nkubectl describe ingress -A\nkubectl describe svc -n ingress-nginx\nkubectl delete configmap nginx-config -n ingress-nginx\nkubectl rollout restart deployment -n NAMESPACE_OF_YOUR_APP\nkubectl get all -A` does not include \"ingress\", so use `kubectl get ing -A\n</code></pre>"},{"location":"Setting_up_a_cluster/#useful-links","title":"Useful Links","text":"<ul> <li>kubectl Cheat Sheet</li> <li>A visual guide on troubleshooting Kubernetes deployments</li> </ul>"},{"location":"Storage/","title":"Storage","text":"<p>Once your cluster is set up, you can easily create persistent volumes using the default storage class <code>hcloud-volumes</code>. The Hetzner CSI driver is automatically installed, allowing you to use Hetzner's block storage for these volumes. This storage is based on Ceph, ensuring it\u2019s both replicated and highly available. Keep in mind that the minimum size for a volume is 10Gi. If you try to create a smaller volume, it will still be created with a 10Gi capacity.</p> <p>For workloads that require maximum IOPS, such as databases, there\u2019s also the <code>local-path</code> storage class. This is disabled by default, but you can enable it by setting <code>local_path_storage_class</code>.<code>enabled</code> to <code>true</code> in the configuration file. For more details, refer to this page.</p>"},{"location":"Troubleshooting/","title":"Troubleshooting","text":"<p>If the tool stops working after creating instances and you experience timeouts, the issue might be related to your SSH key. This can happen if you\u2019re using a key with a passphrase or an older key, as newer operating systems may no longer support certain encryption methods.</p> <p>To fix this, you can try enabling <code>networking</code>.<code>ssh</code>.<code>use_agent</code> by setting it to <code>true</code>. This lets the SSH agent manage the key. If you\u2019re not familiar with what an SSH agent does, you can refer to this page for a straightforward explanation.</p> <p>You can also run <code>hetzner-k3s</code> with the <code>DEBUG</code> environment variable set to <code>true</code>. This will provide more detailed output, which can help you identify the root of the problem.</p> <p>In most cases, if you\u2019re able to manually run <code>ssh</code> commands on the servers using a specific keypair, <code>hetzner-k3s</code> should work as well, since it relies on the <code>ssh</code> binary to function.</p>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/","title":"Upgrading a cluster created with hetzner-k3s v1.x to v2.x","text":"<p>The v1 version of hetzner-k3s is quite old and hasn\u2019t been supported for some time. I understand that many haven\u2019t upgraded to v2 because, until now, there wasn\u2019t a simple process to do this.</p> <p>The good news is that the migration is now possible and straightforward, as long as you follow these instructions carefully and take your time. This upgrade also allows you to replace deprecated instance types (like the <code>CX</code> series) with newer ones. Note that this migration requires hetzner-k3s v2.2.4 or higher.</p>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#prerequisites","title":"Prerequisites","text":"<ul> <li> I suggest installing the hcloud utility. It will make it easier and faster to delete old master nodes.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#upgrading-configuration-and-first-steps","title":"Upgrading configuration and first steps","text":"<ul> <li> Backup apps and data \u2013 As with any migration, there\u2019s some risk involved, so it\u2019s better to be prepared in case things don\u2019t go as planned.</li> <li> Backup kubeconfig and the old config file</li> <li> Uninstall the System Upgrade Controller</li> <li> Create a resolv file on existing nodes. You can do this manually or automate it using the <code>hcloud</code> CLI: <pre><code>hcloud server list | awk '{print $4}' | tail -n +2 | while read ip; do\n  echo \"Setting DNS for ${ip}\"\n  ssh -n root@${ip} \"echo nameserver 8.8.8.8 | tee /etc/k8s-resolv.conf\"\n  ssh -n root@${ip} \"cat /etc/k8s-resolv.conf\"\ndone\n</code></pre></li> <li> Convert the config file to the new format. You can find guidance here.</li> <li> Remove or comment out empty node pools from the config file.</li> <li> Set <code>embedded_registry_mirror</code>.<code>enabled</code> to <code>false</code> if necessary, depending on the current version of k3s (refer to this documentation).</li> <li> Add <code>legacy_instance_type</code> to ALL node pools, including both masters and workers. Set it to the current instance type (even if it\u2019s deprecated). This step is critical for the migration.</li> <li> Run the <code>create</code> command using the latest version of hetzner-k3s and the new config file.</li> <li> Wait for all CSI pods in <code>kube-system</code> to restart, and make sure everything is running correctly.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#rotating-control-plane-instances-with-the-new-instance-type","title":"Rotating control plane instances with the new instance type","text":"<p>Replace one master at a time (unless your cluster has a load balancer for the Kubernetes API, switch to another master's kube context before replacing <code>master1</code>):</p> <ul> <li> Drain and delete the master using both kubectl and the Hetzner console (or the <code>hcloud</code> CLI) to remove the actual instance.</li> <li> Rerun the <code>create</code> command to recreate the master with the new instance type. Wait for it to join the control plane and reach the \"ready\" status.</li> <li> SSH into each master and verify that the etcd members are updated and in sync:</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install etcd-client\n\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=https://127.0.0.1:2379\nexport ETCDCTL_CACERT=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt\nexport ETCDCTL_CERT=/var/lib/rancher/k3s/server/tls/etcd/server-client.crt\nexport ETCDCTL_KEY=/var/lib/rancher/k3s/server/tls/etcd/server-client.key\n\netcdctl member list\n</code></pre> <p>Repeat this process carefully for each master. After all three masters have been replaced:</p> <ul> <li> Rerun the <code>create</code> command once or twice to ensure the configuration is stable and the masters no longer restart.</li> <li> Debug DNS resolution. If there are issues, restart the agents for DNS resolution with this command, then restart CoreDNS: <pre><code>hcloud server list | grep worker | awk '{print $4}'| while read ip; do\n  echo \"${ip}\"\n  ssh -n root@${ip} \"systemctl restart k3s-agent\"\n  sleep 10\ndone\n</code></pre></li> <li> Address any issues with your workloads before proceeding to rotate the worker nodes.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#rotating-a-worker-node-pool","title":"Rotating a worker node pool","text":"<ul> <li> Increase the node count for the pool by 1.</li> <li> Run the <code>create</code> command to create the extra node needed during the pool rotation.</li> </ul> <p>Replace one worker node at a time (except for the last one you just added):</p> <ul> <li> Drain a node.</li> <li> Delete the drained node using both kubectl and the Hetzner console (or the <code>hcloud</code> CLI).</li> <li> Rerun the <code>create</code> command to recreate the deleted node.</li> <li> Verify everything is working as expected before moving on to the next node in the pool.</li> </ul> <p>Once all the existing nodes have been rotated:</p> <ul> <li> Drain the very last node in the pool (the one you added earlier).</li> <li> Verify everything is functioning correctly.</li> <li> Delete the last node using both kubectl and the Hetzner console (or the <code>hcloud</code> CLI).</li> <li> Update the <code>instance_count</code> for the node pool by reducing it by 1.</li> <li> Proceed with the next pool.</li> </ul>"},{"location":"Upgrading_a_cluster_from_1x_to_2x/#finalizing","title":"Finalizing","text":"<ul> <li> Remove the <code>legacy_instance_type</code> setting from both master and worker node pools.</li> <li> Rerun the <code>create</code> command once more to double-check everything.</li> <li> Optionally, convert the currently zonal cluster to a regional one with masters in different locations (see this guide).</li> </ul>"}]}